{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bbef98-d83a-48a6-bc64-72dd95ac14d8",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d65a0-fe6f-45ea-87e5-e5a5218decda",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "WordNet is a legical database of English that provides a hierarchy of the English language. WordNet allows us to understand the relationships between words, with relationships ranging from the basics (synonyms, antonyms), to those unique to WordNet's hierarchy of lamguage (hypernyms, hyponyms, meronyms, holonyms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956900d0-c14f-4b85-83e7-7c85f72f6000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e1d0e-0bc6-4b74-bd7e-d3ad3a0705ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Usage on nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d652e19-8151-44de-8800-de359f76d81c",
   "metadata": {},
   "source": [
    "Use the word \"guitar\" as the noun in question. Print all synsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea9e370-6dda-49a1-b84f-70cf7f7fcfea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('guitar.n.01')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('guitar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b2a10-f2bf-48c8-aac8-c7eac6026e02",
   "metadata": {},
   "source": [
    "Select `guitar.n.01` as the synset to use. Print definition, examples, lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec9a8650-ba92-4668-b6e8-835866031094",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: Synset('guitar.n.01')\n",
      "Definition: a stringed instrument usually having six strings; played by strumming or plucking\n",
      "\n",
      "Examples: []\n",
      "\n",
      "Lemmas: [Lemma('guitar.n.01.guitar')]\n"
     ]
    }
   ],
   "source": [
    "ss = wn.synset('guitar.n.01')\n",
    "\n",
    "print(f'Synset: {ss}')\n",
    "print(f'Definition: {ss.definition()}\\n')\n",
    "print(f'Examples: {ss.examples()}\\n')\n",
    "print(f'Lemmas: {ss.lemmas()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6542b96-61ee-4f23-a46a-d75f5b4dbcc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Traverse up the WordNet hierarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fde387-dcf3-4218-9256-89ad00ea9402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def traverse_hierarchy(start):\n",
    "    print(f'Starting from {start} and going up')\n",
    "    hyp = start.hypernyms()[0]\n",
    "    while hyp:\n",
    "        print(f'\\tSynset: {hyp}')\n",
    "        if hyp.hypernyms():\n",
    "            hyp = hyp.hypernyms()[0]\n",
    "        else:\n",
    "            break\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51888486-3734-4330-a5ba-a49c64846386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from Synset('guitar.n.01') and going up\n",
      "\tSynset: Synset('stringed_instrument.n.01')\n",
      "\tSynset: Synset('musical_instrument.n.01')\n",
      "\tSynset: Synset('device.n.01')\n",
      "\tSynset: Synset('instrumentality.n.03')\n",
      "\tSynset: Synset('artifact.n.01')\n",
      "\tSynset: Synset('whole.n.02')\n",
      "\tSynset: Synset('object.n.01')\n",
      "\tSynset: Synset('physical_entity.n.01')\n",
      "\tSynset: Synset('entity.n.01')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "traverse_hierarchy(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a240ed3a-3975-46af-87e5-6e9aa86dee98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from Synset('ghost.n.01') and going up\n",
      "\tSynset: Synset('apparition.n.03')\n",
      "\tSynset: Synset('illusion.n.01')\n",
      "\tSynset: Synset('appearance.n.04')\n",
      "\tSynset: Synset('representation.n.01')\n",
      "\tSynset: Synset('content.n.05')\n",
      "\tSynset: Synset('cognition.n.01')\n",
      "\tSynset: Synset('psychological_feature.n.01')\n",
      "\tSynset: Synset('abstraction.n.06')\n",
      "\tSynset: Synset('entity.n.01')\n",
      "---\n",
      "Starting from Synset('computer.n.01') and going up\n",
      "\tSynset: Synset('machine.n.01')\n",
      "\tSynset: Synset('device.n.01')\n",
      "\tSynset: Synset('instrumentality.n.03')\n",
      "\tSynset: Synset('artifact.n.01')\n",
      "\tSynset: Synset('whole.n.02')\n",
      "\tSynset: Synset('object.n.01')\n",
      "\tSynset: Synset('physical_entity.n.01')\n",
      "\tSynset: Synset('entity.n.01')\n",
      "---\n",
      "Starting from Synset('idea.n.01') and going up\n",
      "\tSynset: Synset('content.n.05')\n",
      "\tSynset: Synset('cognition.n.01')\n",
      "\tSynset: Synset('psychological_feature.n.01')\n",
      "\tSynset: Synset('abstraction.n.06')\n",
      "\tSynset: Synset('entity.n.01')\n",
      "---\n",
      "Starting from Synset('sandwich.n.01') and going up\n",
      "\tSynset: Synset('snack_food.n.01')\n",
      "\tSynset: Synset('dish.n.02')\n",
      "\tSynset: Synset('nutriment.n.01')\n",
      "\tSynset: Synset('food.n.01')\n",
      "\tSynset: Synset('substance.n.07')\n",
      "\tSynset: Synset('matter.n.03')\n",
      "\tSynset: Synset('physical_entity.n.01')\n",
      "\tSynset: Synset('entity.n.01')\n",
      "---\n",
      "Starting from Synset('jersey.n.03') and going up\n",
      "\tSynset: Synset('shirt.n.01')\n",
      "\tSynset: Synset('garment.n.01')\n",
      "\tSynset: Synset('clothing.n.01')\n",
      "\tSynset: Synset('consumer_goods.n.01')\n",
      "\tSynset: Synset('commodity.n.01')\n",
      "\tSynset: Synset('artifact.n.01')\n",
      "\tSynset: Synset('whole.n.02')\n",
      "\tSynset: Synset('object.n.01')\n",
      "\tSynset: Synset('physical_entity.n.01')\n",
      "\tSynset: Synset('entity.n.01')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# try examples that aren't in the instructions\n",
    "traverse_hierarchy(wn.synset('ghost.n.01'))\n",
    "traverse_hierarchy(wn.synset('computer.n.01'))\n",
    "traverse_hierarchy(wn.synset('idea.n.01'))\n",
    "traverse_hierarchy(wn.synset('sandwich.n.01'))\n",
    "traverse_hierarchy(wn.synset('jersey.n.03'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b0c88-9203-423d-b80a-a9c9ae4d921a",
   "metadata": {},
   "source": [
    "WordNet is organized via synonyms, called *synsets*. Antonyms are defined, if they exist. In terms of hierarchy, we have hypernyms and hyponyms, where holonyms are a lower version of the word and hypernyms is a \"higher\" version of the word.\n",
    "\n",
    "Based on these five traversals, we can see that all nouns \"inherit\" from `entity.n.01`. (By this, I mean that all nouns have a hypernym of \"entity\"). Depending on what the noun is, it either is a physical entity or an abstraction, and then splits off into the proper subcategory. Physical things seem to be classified as physical entities, and then are broken down as matter or objects, and then are drilled down into their respective categories. Abstractions are usually broken down into psychological features and focus on other abstract concepts to break these abstractions down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6ee1b-6cac-42c4-a589-7fbebc66abdf",
   "metadata": {},
   "source": [
    "Print hypernyms, hyponyms, meronyms, holonyms, antonyms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1701c417-c7f1-42b6-9d68-bdf79828dc71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: Synset('guitar.n.01')\n",
      "\tHypernyms: [Synset('stringed_instrument.n.01')]\n",
      "\tHyponyms: [Synset('acoustic_guitar.n.01'), Synset('bass_guitar.n.01'), Synset('cittern.n.01'), Synset('electric_guitar.n.01'), Synset('hawaiian_guitar.n.01'), Synset('uke.n.01')]\n",
      "\tMeronyms: [Synset('fingerboard.n.03')]\n",
      "\tHolonyms: []\n",
      "\tAntonyms: []\n"
     ]
    }
   ],
   "source": [
    "print(f'Synset: {ss}')\n",
    "\n",
    "# call method if exists on object else return empty list\n",
    "# cfe = call function if exists!\n",
    "cfe = lambda obj, attr: getattr(obj, attr)() if hasattr(obj, attr) else []\n",
    "\n",
    "print(f'\\tHypernyms: {cfe(ss, \"hypernyms\")}')\n",
    "print(f'\\tHyponyms: {cfe(ss, \"hyponyms\")}')\n",
    "print(f'\\tMeronyms: {cfe(ss, \"part_meronyms\")}')\n",
    "print(f'\\tHolonyms: {cfe(ss, \"member_holonyms\")}')\n",
    "print(f'\\tAntonyms: {cfe(ss, \"antonyms\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc1da1-53d2-446a-9636-398ba5f298e7",
   "metadata": {},
   "source": [
    "## Usage on verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d99c4-ff21-4a5f-b173-20b927a539c6",
   "metadata": {},
   "source": [
    "Use the word \"gagged\" as the verb in question. Print all synsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b239fb-4653-40fc-970e-d453e58c981e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('gag.v.01'),\n",
       " Synset('choke.v.02'),\n",
       " Synset('gag.v.03'),\n",
       " Synset('gag.v.04'),\n",
       " Synset('gag.v.05'),\n",
       " Synset('gag.v.06'),\n",
       " Synset('gag.v.07')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb = 'gagged'\n",
    "wn.synsets(verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef4edc5-0a48-4220-a399-0d44dd41c1b3",
   "metadata": {},
   "source": [
    "Select `gag.v.03` as the synset to use. Print definition, examples, lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d71b97-cf3f-4445-9621-ca67f5146c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: Synset('gag.v.03')\n",
      "-----\n",
      "\n",
      "Definition: tie a gag around someone's mouth in order to silence them\n",
      "\n",
      "Examples: ['The burglars gagged the home owner and tied him to a chair']\n",
      "\n",
      "Lemmas: [Lemma('gag.v.03.gag'), Lemma('gag.v.03.muzzle')]\n"
     ]
    }
   ],
   "source": [
    "st = wn.synset('gag.v.03')\n",
    "\n",
    "print(f'Synset: {st}\\n-----\\n')\n",
    "print(f'Definition: {st.definition()}\\n')\n",
    "print(f'Examples: {st.examples()}\\n')\n",
    "print(f'Lemmas: {st.lemmas()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6096c-48aa-481b-afc1-9cdab9867b00",
   "metadata": {},
   "source": [
    "Traverse up the WordNet hierarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa7f2eb8-3116-4210-be77-d9972eab3bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from Synset('gag.v.03') and going up\n",
      "\tSynset: Synset('tie.v.01')\n",
      "\tSynset: Synset('fasten.v.01')\n",
      "\tSynset: Synset('attach.v.01')\n",
      "\tSynset: Synset('connect.v.01')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "traverse_hierarchy(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a497a5-0c48-4b61-a060-b73aaa508dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from Synset('run.v.01') and going up\n",
      "\tSynset: Synset('travel_rapidly.v.01')\n",
      "\tSynset: Synset('travel.v.01')\n",
      "---\n",
      "Starting from Synset('question.v.01') and going up\n",
      "\tSynset: Synset('challenge.v.02')\n",
      "\tSynset: Synset('invite.v.04')\n",
      "\tSynset: Synset('request.v.02')\n",
      "\tSynset: Synset('ask.v.02')\n",
      "\tSynset: Synset('request.v.01')\n",
      "\tSynset: Synset('communicate.v.01')\n",
      "\tSynset: Synset('convey.v.03')\n",
      "\tSynset: Synset('transfer.v.02')\n",
      "\tSynset: Synset('move.v.02')\n",
      "---\n",
      "Starting from Synset('imagine.v.01') and going up\n",
      "\tSynset: Synset('create_by_mental_act.v.01')\n",
      "\tSynset: Synset('make.v.03')\n",
      "---\n",
      "Starting from Synset('punch.v.01') and going up\n",
      "\tSynset: Synset('hit.v.03')\n",
      "\tSynset: Synset('touch.v.01')\n",
      "---\n",
      "Starting from Synset('think.v.01') and going up\n",
      "\tSynset: Synset('evaluate.v.02')\n",
      "\tSynset: Synset('think.v.03')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# try examples that aren't in the instructions\n",
    "traverse_hierarchy(wn.synset('run.v.01'))\n",
    "traverse_hierarchy(wn.synset('question.v.01'))\n",
    "traverse_hierarchy(wn.synset('ideate.v.01'))\n",
    "traverse_hierarchy(wn.synset('punch.v.01'))\n",
    "traverse_hierarchy(wn.synset('think.v.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b5853-7ae3-46d8-ba2f-99afc70b9a4a",
   "metadata": {},
   "source": [
    "Verbs use a similar organizations as nouns. But, based on these five traversals, we can see that not all verbs inherit from a common ancestor (hypernym), like nouns do. As we move up the hierarchy, it gets reduced down into multiple different generalized nouns. We can also see that some synsets don't correlate with a specific word, but a phrase. For instance, `create_by_mental_act.v.01` isn't a word, but describes a specific category that can't be expressed via a singular word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77172f9d-34cf-41a7-8883-83ba35271846",
   "metadata": {},
   "source": [
    "Find all forms of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8243b9d-6904-418a-b811-df44db76f79e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All forms of word \"gagged\": {'gag', 'muzzle', 'heave', 'fret', 'choke', 'quip', 'strangle', 'suffocate', 'retch'}\n"
     ]
    }
   ],
   "source": [
    "all_forms = set([])\n",
    "# ml = morphize lemma\n",
    "ml = lambda lemma: wn.morphy(lemma.name())\n",
    "\n",
    "for synset in wn.synsets(verb):\n",
    "    all_forms.update([ml(l) for l in synset.lemmas()])\n",
    "\n",
    "print(f'All forms of word \"{verb}\": {all_forms}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3982dba-1abb-463b-bb82-0c30346dcc51",
   "metadata": {},
   "source": [
    "Select similar words and run Wu-Palmer and Lesk on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9568aa8-553e-4e01-a404-57387299a98a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word A synsets: [Synset('suck.v.01'), Synset('suck.v.02'), Synset('suck.v.03'), Synset('suck.v.04'), Synset('fellate.v.01'), Synset('absorb.v.04'), Synset('breastfeed.v.01')]\n",
      "Word B synsets: [Synset('drink.v.01'), Synset('drink.v.02'), Synset('toast.v.02'), Synset('drink_in.v.01'), Synset('drink.v.05')]\n",
      "\n",
      "\n",
      "Wu-Palmer similarity: 0.8\n",
      "Lesk for sentence ['She', 'sucked', 'on', 'the', 'straw', 'but', 'the', 'milkshake', 'was', 'frozen']: Synset('suck.v.01')\n",
      "Lesk for sentence ['Sarah', 'drank', 'her', 'smoothie']: Synset('toast.v.02')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "wordA = wn.synsets(\"sucked\")\n",
    "wordB = wn.synsets(\"drank\")\n",
    "\n",
    "print(f'Word A synsets: {wordA}')\n",
    "print(f'Word B synsets: {wordB}\\n\\n')\n",
    "\n",
    "# pick closest synset\n",
    "sA = wn.synset('suck.v.01')\n",
    "sB = wn.synset('drink.v.01')\n",
    "\n",
    "# lesk stuff\n",
    "sentA = 'She sucked on the straw but the milkshake was frozen'.split(' ')\n",
    "leskA = lesk(sentA, 'sucked', 'v')\n",
    "sentB = 'Sarah drank her smoothie'.split(' ')\n",
    "leskB = lesk(sentB, 'drank', 'v')\n",
    "\n",
    "print(f'Wu-Palmer similarity: {sA.wup_similarity(sB)}')\n",
    "print(f'Lesk for sentence {sentA}: {leskA}')\n",
    "print(f'Lesk for sentence {sentB}: {leskB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ff9f7-41a3-4f8c-9a20-77822a6a0a28",
   "metadata": {},
   "source": [
    "The Wu-Palmer similarity algorithm uses synsets to determine how similar two words are. In this case, we used the words \"sucked\" and \"drank\", both of which discuss intake of a liquid. Because they share a high degree of hypernyms, this metric tells us that these words are 80% similar.\n",
    "\n",
    "On the other hand, the Lesk algorithm focuses on word sense disambiguation based on the dictionary glosses to see how many words overlap. This algorithm worked well for the word \"sucked,\" with it picking up that `suck.v.01` is the sense of the word. However, this algorithm didn't work as well for the word \"drank,\" with it picking up `toast.v.02` (which refers to making a toast in a drink setting) as the sense of the word. This demonstrates the imperfect nature of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289c6e9-0a5b-4cbd-847a-1e23fa728dbb",
   "metadata": {},
   "source": [
    "# SentiWordNet\n",
    "\n",
    "SentiWordNet builds on top of WordNet to allow for sentiment to be gleaned from text. Each synset in the database is assigned a positive, negative, and objectivity score. The sum of these three add up into 1.\n",
    "\n",
    "You could use SentiWordNet to calculate the polarity of a sentence or a larger text, or to track the shift in polarity of a text as it progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9cfadc3-2b52-45f4-ab6f-3f1524ac7732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word is \"fucker\"\n",
      "Scores for word fucker.n.01 - Positive: 0.125 Negative: 0.0 Objective: 0.875\n",
      "Scores for word fucker.n.02 - Positive: 0.0 Negative: 0.25 Objective: 0.75\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def print_scores(senti_synset):\n",
    "    pos_s, neg_s, obj_s = senti_synset.pos_score(), senti_synset.neg_score(), senti_synset.obj_score()\n",
    "    print(f'Scores for word {senti_synset.synset.name()} -', end=\" \")\n",
    "    print(f'Positive: {pos_s}', end=\" \")\n",
    "    print(f'Negative: {neg_s}', end=\" \")\n",
    "    print(f'Objective: {obj_s}')\n",
    "\n",
    "# emotionally charged word\n",
    "em_word = 'fucker'\n",
    "em_synsets = wn.synsets(em_word)\n",
    "# selected synset\n",
    "em_sel_synset = em_synsets[1]\n",
    "print(f'The word is \"{em_word}\"')\n",
    "# scoring\n",
    "scores = swn.senti_synsets('fucker', 'n')\n",
    "for score in scores:\n",
    "    print_scores(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79b62f5f-07e7-4b4c-8681-f4d31366fea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring word \"League\"\n",
      "\n",
      "Scores for word league.n.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word league.n.02 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word league.n.03 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word league.v.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "---\n",
      "\n",
      "Scoring word \"of\"\n",
      "\n",
      "No score available\n",
      "---\n",
      "\n",
      "Scoring word \"Legends\"\n",
      "\n",
      "Scores for word legend.n.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word caption.n.03 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "---\n",
      "\n",
      "Scoring word \"is\"\n",
      "\n",
      "Scores for word be.v.01 - Positive: 0.25 Negative: 0.125 Objective: 0.625\n",
      "Scores for word be.v.02 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word be.v.03 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word exist.v.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word be.v.05 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word equal.v.01 - Positive: 0.125 Negative: 0.125 Objective: 0.75\n",
      "Scores for word constitute.v.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word be.v.08 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word embody.v.02 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word be.v.10 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word be.v.11 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word be.v.12 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word cost.v.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "---\n",
      "\n",
      "Scoring word \"a\"\n",
      "\n",
      "Scores for word angstrom.n.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word vitamin_a.n.01 - Positive: 0.125 Negative: 0.25 Objective: 0.625\n",
      "Scores for word deoxyadenosine_monophosphate.n.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word adenine.n.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word ampere.n.02 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word a.n.06 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word a.n.07 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "---\n",
      "\n",
      "Scoring word \"terrible\"\n",
      "\n",
      "Scores for word awful.s.02 - Positive: 0.0 Negative: 0.625 Objective: 0.375\n",
      "Scores for word atrocious.s.02 - Positive: 0.0 Negative: 0.875 Objective: 0.125\n",
      "Scores for word severe.s.01 - Positive: 0.0 Negative: 0.875 Objective: 0.125\n",
      "Scores for word frightful.s.02 - Positive: 0.125 Negative: 0.25 Objective: 0.625\n",
      "---\n",
      "\n",
      "Scoring word \"and\"\n",
      "\n",
      "No score available\n",
      "---\n",
      "\n",
      "Scoring word \"incomplete\"\n",
      "\n",
      "Scores for word incomplete.a.01 - Positive: 0.5 Negative: 0.25 Objective: 0.25\n",
      "Scores for word incomplete.s.02 - Positive: 0.0 Negative: 0.125 Objective: 0.875\n",
      "---\n",
      "\n",
      "Scoring word \"game\"\n",
      "\n",
      "Scores for word game.n.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word game.n.02 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word game.n.03 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word game.n.04 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word game.n.05 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word game.n.06 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word game.n.07 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word plot.n.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word game.n.09 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word game.n.10 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word game.n.11 - Positive: 0.25 Negative: 0.5 Objective: 0.25\n",
      "Scores for word bet_on.v.01 - Positive: 0.0 Negative: 0.0 Objective: 1.0\n",
      "Scores for word crippled.s.01 - Positive: 0.0 Negative: 0.5 Objective: 0.5\n",
      "Scores for word game.s.02 - Positive: 0.125 Negative: 0.375 Objective: 0.5\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_polarity(word):\n",
    "    print(f'Scoring word \"{word}\"\\n')\n",
    "    scores = list(swn.senti_synsets(word))\n",
    "    if scores:\n",
    "        for score in scores:\n",
    "            print_scores(score)\n",
    "    if not scores or len(scores) == 0:\n",
    "        print('No score available')\n",
    "    print('---\\n')\n",
    "\n",
    "\n",
    "sentence = 'League of Legends is a terrible and incomplete game'.split(' ')\n",
    "for word in sentence:\n",
    "    print_polarity(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6019d93-be2a-4656-8ad3-a7ae34bfacb8",
   "metadata": {},
   "source": [
    "In the sentence example, we can see how words that are related but aren't even the exact word appear in the senti-synsets, likely because they were identified as synsets by WordNet. We can also see that even in emotionally charged words such as \"awful,\" the algorithm balances the negative sentiment with the potential objectivity of the term (something can be objectively terrible). Interestingly, in the word example, it classified the word \"fucker\" as mostly objective rather than negative, even though that word is usually not used in an objective context.\n",
    "\n",
    "SentiWordNet can be used in NLP applications to classify any sort of text, which in turn could be used to automate tasks like sentiment analysis or opinion mining, which are commonly used in social media monitoring, market research, and customer feedback analysis.\n",
    "\n",
    "Even beyond that, the sentiment of a text could potentially be used to crudely identify the topic of the text. For instance, a news article that contains mostly negative sentiment words might be classified as a story about a natural disaster or a crime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9721f9-cecd-4760-98cf-cd0b6c46e156",
   "metadata": {},
   "source": [
    "# Collocations\n",
    "\n",
    "Collocations refer to a group of words that take on a new meaning, and cannot be broken into their constituent parts when analyzing a piece of text. For instance, the colloquial college term \"jungle juice\" does not refer to juice from the jungle. Collocations can be found by calculating the chances of two words being together versus their probability appearing in general. More specifically, this calculation is the base-2 log of the probability of finding word A and B together, divided by the quantity of probability of finding A times probability of finding B. This is called a PMI, or point-wise mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b14f25-7dc8-499b-a92b-84f99697010c",
   "metadata": {},
   "source": [
    "Print collocations for the inaugural address corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c00e03e-f272-4f0a-bb18-67a5826fa80c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "text4.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfc83d-5fb8-4baf-91a5-5d6123d84fd8",
   "metadata": {},
   "source": [
    "Calculate mutual information for `Indian tribes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f412012c-d7bc-40b8-b4c1-eab81530c25a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = \"Indian\", y = \"tribes\", p(x) = 0.13095238095238096, p(y) = 0.07142857142857142, p(xy) = 0.07142857142857142\n",
      "pmi calculation for \"Indian tribes\" = 2.932885804141463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def pmi_calc(words, text):\n",
    "    vocab = len(set(all_text))\n",
    "    split = words.split(' ')\n",
    "    x, y = split[0], split[1]\n",
    "    xy = words\n",
    "    if len(split) != 2:\n",
    "        raise ValueError('Provided too many words for a collocation')\n",
    "    p_xy = text.count(xy) / vocab\n",
    "    p_x = text.count(x) / vocab\n",
    "    p_y = text.count(y) / vocab\n",
    "    pmi = math.log2(p_xy / (p_x * p_y))\n",
    "    print(f'x = \"{x}\", y = \"{y}\", p(x) = {p_x}, p(y) = {p_y}, p(xy) = {p_xy}')\n",
    "    return pmi\n",
    "\n",
    "\n",
    "\n",
    "all_text = ' '.join(text4.tokens)\n",
    "coll = 'Indian tribes'\n",
    "print(f'pmi calculation for \"{coll}\" = {pmi_calc(coll, all_text)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7775a1-dcaa-4837-99bc-60d171fedd06",
   "metadata": {},
   "source": [
    "Looking at the individual values for probability:\n",
    "    - \"Indian\" is about 13% likely to appear in the text\n",
    "    - \"tribes\" is about 7% likely to appear in the text\n",
    "    - Together, they are about 7% likely to appear in the text.\n",
    "    \n",
    "This means that the word \"tribes\" is most likely accompanied by \"Indian,\" while there still existing cases where \"Indian\" will appear alone.\n",
    "\n",
    "Computing PMI for this phrase gave us a positive value. This indicates that the co-occurrence of these two words is more frequent than would be expected by chance, providing strong evidence that they form a collocation within this corpus.\n",
    "\n",
    "We can then conclude that these words have a strong chance of appearing together within this corpus. As such, this gives us a method for narrowing down and analyzing potential strong collocations that we can explore further, which can only get better with larger corpus sizes and text variety."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Aditya Rathod (AGR190000)"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "title": "CS 4395 Homework 3: WordNet"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
